

## Week 5: gradient descent in practice

### The Plan:
I need to put the things I learned about Tensorflow Autodifferentiation into practice. I need to practice using the TF library to reinforce my understanding of it. To do this I need a simple problem. After speaking with professor Bernardo, we decided to use the polynomial
$$
2 * x**6 + 2 * x**5 - 2 * x**4 - x**3 + 2 
$$
to construct the trainable model and implement gradient descent. 

Considering the polynomial is simple, I had to verify this method would work on the more complex exponential function we derived. Once gradient descent is successful on the simple function, I will attempt to add a few additional variables to it to simulate the exponential function.

Now that I have a concrete problem to solve, I need to identify the different parts of it.
I need perform gradient descent I need do this until the output of the function has reached a local minima

### The Implementation: 
While implementing the gradient descent I ran into an issue, I was only getting one output. I was expecting the output of 
```Python
with tf.GradientTape() as tape:
    y = l(x)
```
to be the full descent. Upon examination of the documentation, I noticed there was no mention of a loop that updated our values. Considering my next meeting with professor Bernardo is soon, I will bring this up to him.


<img width="445" height="324" alt="image" src="https://github.com/user-attachments/assets/cf51dbb6-2c58-4b68-b92b-1694d188b894" />
