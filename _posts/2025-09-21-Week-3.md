## Week 3: Fitting the 2d Gaussian function

I was spot on with my understanding of the next goal. During our meeting the professor simplified the 2d gaussian equation down to isolate parameters we would need. Connecting them to a graph helped me visualized the entire problem -- keeping the small details of the model and the bigger picture in mind. 

Once the equation was simplified, mean squared error was introduced. It's a reletively simple concept -- the distance between the values of our actual (which?) data and the gaussian function needs to be as small as possible. How do we accomplish this? the option professor Bernardo suggested was gradient descent. Gradient descent is a method of "training" that follows the slope of a curve to some local minima. To follow the slope of the curve a tangential value needs to be extracted and evaluated to the actual data. In ML this is typically called a [forward pass](https://www.geeksforgeeks.org/deep-learning/what-is-forward-propagation-in-neural-networks/). 

When finding the partial derivitive for m1 I realized I struggle to identify parts that reduced to a constant, and what gets dropped for the next layer of the chain rule. After doing a little bit of [research](https://www.youtube.com/watch?v=JAf_aSIJryg) any variable being differentiated with respect to was treated as a constant, and any outer function was treated as $\frac{d}{dy}y^2 = 2x$ where $x = y$. In this manner I was able to find $\partial m_1 \ell = \sum_{i=1}^6 2(f(x_i,y_i)-z_i) * aexp(b|| x-m_1 ||^2) * 2b(m-x_1)$

I started looking into Tensorflow Autodifferentiation. It uses GradientTapes as a means of backpropigation. They exists to remember the operations done during a forward pass in training. To be able to use them I will need to create a Tensorflow module representing my gaussian model. The module will act as a variable state object containing the various peices of my model. In order to "train" the model I will have to add the parameters I am trying to find as watched tf.Variables.


